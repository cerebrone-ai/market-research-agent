This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-02T15:16:40.376Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
main.py
src/config.py
src/models/research_models.py
src/services/document_service.py
src/services/llm_service.py
src/workflow/graph.py
src/workflow/nodes.py

================================================================
Repository Files
================================================================

================
File: .gitignore
================
storm/
.storm/
env/
.env
.reports
reports/

*.env
.env
.env.local
.env.development
.env.production

__pycache__/
*.py[cod]
*$py.class

src/models/__pycache__/
services/__pycache__/
workflow/__pycache__/

================
File: main.py
================
import asyncio
from src.services.document_service import DocumentService
from src.workflow.graph import ResearchWorkflow
from dotenv import load_dotenv
import os


load_dotenv()

async def generate_report_for_topic(topic: str):
    try:
        print(f"Starting research on topic: {topic}")
        
        workflow = ResearchWorkflow()
        initial_state = {
            "topic": topic,
            "outline": None,
            "research_results": {},
            "wiki_content": {},
            "final_report": ""
        }
        
        config = {"configurable": {"thread_id": "research-thread"}}
        final_state = await workflow.workflow.ainvoke(initial_state, config)
        
        print("\n=== FINAL REPORT ===\n")
        print(final_state["final_report"])
        
        filename = await DocumentService.save_to_word(topic, final_state["final_report"])
        return filename
        
    except Exception as e:
        print(f"An error occurred while generating report for '{topic}': {str(e)}")
        raise e

async def main():
    topic = "Target Customers Based on below factors every Month in Dilworth, Myers Park, or South End. :  Age: Target age groups likely to value eco-friendly products, such as Millennials (25–40 years old) and Gen X (40–55 years old).Income Level: Middle to upper-income households who can afford premium eco-friendly cleaning services.Family Size: Families with children, as parents often prefer safer, non-toxic cleaning alternatives.Lifestyle Preferences: Individuals who prioritize sustainability, green living, and health-conscious decisions.Values: Customers who care about reducing their carbon footprint and supporting eco-friendly businesses.Pain Points: Concerns about harmful chemicals in traditional cleaning products or interest in supporting local businesses.Study your competitors to understand their customer base and identify gaps in the market. Look at their reviews, pricing, services, and customer feedback.Review our Customer feedback and gather insights into their cleaning needs, concerns, and preferences"
    await generate_report_for_topic(topic)

if __name__ == "__main__":
    asyncio.run(main())

================
File: src/config.py
================
import os

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')
#os.environ["TAVILY_API_KEY"] = "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"

# LLM Models
FAST_LLM_MODEL = "gpt-3.5-turbo-0125"
LONG_CONTEXT_MODEL = "gpt-4-turbo-preview"
EMBEDDINGS_MODEL = "text-embedding-3-small"

REPORTS_DIR = "reports"

================
File: src/models/research_models.py
================
from typing import List, TypedDict
from pydantic import BaseModel, Field

class Subsection(BaseModel):
    subsection_title: str = Field(..., title="Title of the subsection")
    description: str = Field(..., title="Content of the subsection")

    @property
    def as_str(self) -> str:
        return f"### {self.subsection_title}\n\n{self.description}"

class Section(BaseModel):
    section_title: str = Field(..., title="Title of the section")
    description: str = Field(..., title="Content of the section")
    subsections: List[Subsection] = Field(default_factory=list)

    @property
    def as_str(self) -> str:
        subsections = "\n\n".join(s.as_str for s in self.subsections)
        return f"## {self.section_title}\n\n{self.description}\n\n{subsections}"

class Outline(BaseModel):
    page_title: str = Field(..., title="Title of the research page")
    sections: List[Section] = Field(default_factory=list)

    @property
    def as_str(self) -> str:
        sections = "\n\n".join(section.as_str for section in self.sections)
        return f"# {self.page_title}\n\n{sections}"

class SearchTerms(BaseModel):
    main_terms: List[str] = Field(..., description="Primary search terms")
    related_terms: List[str] = Field(..., description="Related/secondary search terms")

class ResearchState(TypedDict):
    topic: str
    search_terms: SearchTerms
    outline: Outline
    research_results: dict
    wiki_content: dict
    final_report: str

================
File: src/services/document_service.py
================
import os
import datetime
from docx.api import Document
from docx.shared import Pt
from docx.shared import Inches
from ..config import REPORTS_DIR

class DocumentService:
    @staticmethod
    def add_section_with_style(doc, content: str, level: int):
        """Add a section with consistent styling."""
        clean_content = (
            content.replace('*', '')
            .replace('_', '')
            .replace('####', '')
            .replace('###', '')
            .replace('##', '')
            .replace('#', '')
            .strip()
        )
        
        if level == 0:
            heading = doc.add_heading(clean_content, level=0)
            heading.alignment = 1
            doc.add_paragraph()
        elif level == 1:  
            doc.add_paragraph()  
            heading = doc.add_heading(clean_content, level=1)
            heading.style.font.size = Pt(16)
            heading.style.font.bold = True
        elif level == 2: 
            heading = doc.add_heading(clean_content, level=2)
            heading.style.font.size = Pt(14)
            heading.style.font.bold = True
        else: 
            para = doc.add_paragraph(clean_content)
            para.paragraph_format.space_after = Pt(12)
            para.paragraph_format.line_spacing = 1.15
            
            if clean_content.lstrip().startswith('-'):
                para.style = 'List Bullet'
                para.paragraph_format.left_indent = Inches(0.25)
                para.paragraph_format.first_line_indent = 0
            else:
                para.paragraph_format.first_line_indent = Inches(0.25)

    @staticmethod
    def truncate_title(title: str, max_length: int = 250) -> str:
        """Truncate title to fit Word document property limits."""
        if len(title) <= max_length:
            return title
        return title[:max_length - 3] + "..."

    @staticmethod
    async def save_to_word(topic: str, content: str) -> str:
        try:
            doc = Document()
            

            styles = doc.styles
            styles['Normal'].font.name = 'Calibri'
            styles['Normal'].font.size = Pt(11)
            styles['Normal'].paragraph_format.space_after = Pt(12)
            styles['Normal'].paragraph_format.line_spacing = 1.15

            doc.core_properties.title = DocumentService.truncate_title(topic)
            doc.core_properties.author = "Market Research Team"

            sections = [s for s in content.split('\n\n') if s.strip()]

            title = sections[0].replace('#', '').strip()
            heading = doc.add_heading(title, level=0)
            heading.alignment = 1
            
    
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            timestamp_para = doc.add_paragraph(f"Generated on: {timestamp}")
            timestamp_para.alignment = 1
            doc.add_page_break()
            

            for section in sections[1:]:
                if section.startswith('## '):

                    doc.add_paragraph()  
                    heading = doc.add_heading(section.replace('#', '').strip(), level=1)
                    heading.style.font.size = Pt(16)
                elif section.startswith('### '):

                    heading = doc.add_heading(section.replace('#', '').strip(), level=2)
                    heading.style.font.size = Pt(14)
                elif section.startswith('- ') or section.startswith('* '):

                    p = doc.add_paragraph(section.lstrip('- *').strip(), style='List Bullet')
                    p.paragraph_format.left_indent = Inches(0.25)
                else:

                    p = doc.add_paragraph(section.strip())
                    p.paragraph_format.first_line_indent = Inches(0.25)
            

            os.makedirs(REPORTS_DIR, exist_ok=True)
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{REPORTS_DIR}/research_report_{timestamp}.docx"
            doc.save(filename)
            return filename
            
        except Exception as e:
            print(f"Error saving to Word document: {str(e)}")
            raise e

================
File: src/services/llm_service.py
================
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.retrievers import WikipediaRetriever
from langchain_community.tools.tavily_search import TavilySearchResults
from ..config import FAST_LLM_MODEL, LONG_CONTEXT_MODEL, EMBEDDINGS_MODEL

class LLMService:
    def __init__(self):
        self.fast_llm = ChatOpenAI(model=FAST_LLM_MODEL)
        self.long_context_llm = ChatOpenAI(model=LONG_CONTEXT_MODEL)
        self.embeddings = OpenAIEmbeddings(model=EMBEDDINGS_MODEL)
        self.wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=3)
        self.tavily_search = TavilySearchResults(max_results=3)

================
File: src/workflow/graph.py
================
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from ..models.research_models import ResearchState
from .nodes import WorkflowNodes

class ResearchWorkflow:
    def __init__(self):
        self.nodes = WorkflowNodes()
        self.workflow = self._create_workflow()

    def _create_workflow(self):
        workflow = StateGraph(ResearchState)
        
        workflow.add_node("generate_search_terms", self.nodes.generate_search_terms)
        workflow.add_node("create_outline", self.nodes.create_outline)
        workflow.add_node("gather_research", self.nodes.gather_research)
        workflow.add_node("write_report", self.nodes.write_report)

        workflow.add_edge(START, "generate_search_terms")
        workflow.add_edge("generate_search_terms", "create_outline")
        workflow.add_edge("create_outline", "gather_research")
        workflow.add_edge("gather_research", "write_report")
        workflow.add_edge("write_report", END)

        return workflow.compile(checkpointer=MemorySaver())

================
File: src/workflow/nodes.py
================
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from ..services.llm_service import LLMService
from ..models.research_models import Outline, SearchTerms

class WorkflowNodes:
    def __init__(self):
        self.llm_service = LLMService()

    async def create_outline(self, state):
        """Create a detailed outline for the research topic."""
        print("Creating outline...")
        direct_gen_outline_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a research specialist. Create a detailed outline for a comprehensive research report.
            The outline MUST follow this exact structure for each section:
            - section_title: Required title for each section (use clear, professional titles)
            - description: Required description of what the section covers
            - subsections: Optional list of subsections, each with subsection_title and description

            Guidelines:
            - Analyze the topic to determine the most relevant sections
            - Create a logical flow of information
            - Include 4-6 main sections that best cover the topic
            - Ensure sections are comprehensive but focused
            - Adapt the structure based on the research needs"""),
            ("user", "Create a research outline for the following topic: {topic}")
        ])
        
        generate_outline = direct_gen_outline_prompt | self.llm_service.fast_llm.with_structured_output(Outline)
        outline = await generate_outline.ainvoke({"topic": state["topic"]})
        return {**state, "outline": outline}

    async def gather_research(self, state):
        """Gather research from multiple sources."""
        print("Gathering research...")
        outline = state["outline"]
        search_terms = state["search_terms"]
        
        search_results = {}
        wiki_content = {}
        
        for section in outline.sections:
            section_results = []
            wiki_docs = []
            
            for term in search_terms.main_terms + search_terms.related_terms:
                search_query = f"{term} {section.section_title}"
                try:
                    results = await self.llm_service.tavily_search.ainvoke(search_query)
                    section_results.extend(results)
                except Exception as e:
                    print(f"Warning: Search error for '{search_query}': {str(e)}")
                    continue
            
            try:
                for term in search_terms.main_terms:
                    query = f"{term} {section.section_title}"
                    try:
                        docs = await self.llm_service.wikipedia_retriever.aget_relevant_documents(query)
                        wiki_docs.extend(docs)
                    except Exception as e:
                        print(f"Warning: Wikipedia error for '{query}': {str(e)}")
                        continue
            except Exception as e:
                print(f"Warning: Wikipedia section error: {str(e)}")
            
            search_results[section.section_title] = section_results
            wiki_content[section.section_title] = [doc.page_content for doc in wiki_docs] if wiki_docs else []
        
        return {
            **state,
            "research_results": search_results,
            "wiki_content": wiki_content
        }

    async def write_report(self, state):
        """Write the final research report."""
        print("Writing report...")
        
 
        clean_title = await self.create_title(state["topic"])
        
        write_section_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a professional research writer. Write a detailed section for a research report.
            Use the provided research and Wikipedia content to create comprehensive, well-structured content.
            
            Formatting rules:
            1. Use only THREE levels of headings:
               - Main sections: ## Section Title
               - Subsections: ### Subsection Title
               - No other heading levels or special characters
            
            2. Content structure:
               - Start with an overview paragraph (no heading needed)
               - Use clear subsection headings
               - End with a "Summary" subsection
            
            3. Formatting:
               - Use single blank lines between sections
               - Use dashes (-) for bullet points only
               - Keep paragraphs to 4-6 sentences
               - No special characters (*, _, #, etc.) except for headings
               - No indentation or manual spacing
            
            4. Language:
               - Use professional, clear language
               - Avoid repetition
               - Use transitional phrases between sections"""),
            ("user", """Topic: {topic}
            Section Title: {section_title}
            Section Description: {section_description}
            Research Results: {research_results}
            Wiki Content: {wiki_content}
            
            Write a detailed section that incorporates this information.""")
        ])
        
        write_section = write_section_prompt | self.llm_service.long_context_llm | StrOutputParser()
        
        sections_content = []
        sections_content.append(f"# {clean_title}\n")
        
        for section in state['outline'].sections:
            sections_content.append(f"\n## {section.section_title}\n")
            section_content = await write_section.ainvoke({
                "topic": state["topic"],
                "section_title": section.section_title,
                "section_description": section.description,
                "research_results": state["research_results"].get(section.section_title, []),
                "wiki_content": state["wiki_content"].get(section.section_title, [])
            })
            sections_content.append(section_content)

        final_report = "\n\n".join(sections_content)
        return {**state, "final_report": final_report, "clean_title": clean_title}

    async def create_title(self, topic: str) -> str:
        title_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a professional editor. Create a clear, concise title from the given research topic. Keep it professional and under 10 words."),
            ("user", "{topic}")
        ])
        
        generate_title = title_prompt | self.llm_service.fast_llm | StrOutputParser()
        clean_title = await generate_title.ainvoke({"topic": topic})
        return clean_title

    async def generate_search_terms(self, state):
        """Generate focused search terms from the topic."""
        print("Generating search terms...")
        
        search_terms_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a research specialist. Create focused search terms for gathering information.
            
            Generate two types of search terms:
            1. Main terms: 3-5 primary search phrases that capture the core research needs
            2. Related terms: 5-7 secondary phrases for broader context
            
            Keep terms clear and specific. Include relevant industry terms, metrics, and key concepts.
            Each term should be 2-5 words long."""),
            ("user", """Research Topic: {topic}
            
            Create search terms that will help gather comprehensive information about this topic.""")
        ])
        
        generate_terms = search_terms_prompt | self.llm_service.fast_llm.with_structured_output(SearchTerms)
        search_terms = await generate_terms.ainvoke({"topic": state["topic"]})
        
        return {**state, "search_terms": search_terms}
